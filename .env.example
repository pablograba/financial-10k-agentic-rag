# ==============================================================================
# Agentic 10-K RAG - Environment Configuration
# ==============================================================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control
# ==============================================================================

# ------------------------------------------------------------------------------
# SEC EDGAR Download Configuration
# ------------------------------------------------------------------------------
# Required for downloading 10-K filings from SEC EDGAR
# See: https://www.sec.gov/os/accessing-edgar-data

# Your company/project name (required by SEC fair access policy)
SEC_COMPANY_NAME="AgenticRAG-10K-Project"

# Your email address (required by SEC fair access policy)
SEC_EMAIL="your.email@example.com"

# Output directory for downloaded 10-K filings
SEC_OUTPUT_DIR="./data/raw_10k"

# ------------------------------------------------------------------------------
# Ollama Configuration (Local LLM & Embeddings)
# ------------------------------------------------------------------------------
# Ollama base URL - use host.docker.internal inside Docker container
OLLAMA_BASE_URL="http://127.0.0.1:11434"
# For Docker: OLLAMA_BASE_URL="http://host.docker.internal:11434"

# LLM model for chat/agent (must be pulled in Ollama first)
# Recommended: llama3.1, llama3.2, mistral, qwen2.5, etc.
OLLAMA_LLM_MODEL="llama3.1:8b"

# Embedding model for vector search (must be pulled in Ollama first)
# Recommended: nomic-embed-text, mxbai-embed-large, snowflake-arctic-embed
OLLAMA_EMBEDDING_MODEL="nomic-embed-text"

# Embedding dimension (must match your chosen model)
# nomic-embed-text: 768
# mxbai-embed-large: 1024
# snowflake-arctic-embed:m: 768
EMBEDDING_DIMENSION=768

# LLM temperature for agent responses (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.1

# Max tokens for LLM responses
LLM_MAX_TOKENS=2048

# ------------------------------------------------------------------------------
# Qdrant Vector Database Configuration
# ------------------------------------------------------------------------------
# Qdrant server URL (required)
QDRANT_URL="http://localhost:6333"

# Qdrant collection name for 10-K chunks
QDRANT_COLLECTION_NAME="sp500_10k_chunks"

# Vector distance metric (Cosine, Euclid, or Dot)
QDRANT_DISTANCE_METRIC="Cosine"

# ------------------------------------------------------------------------------
# Ingestion Pipeline Configuration
# ------------------------------------------------------------------------------
# Chunk size for text splitting (in tokens)
CHUNK_SIZE=600

# Chunk overlap (in tokens)
CHUNK_OVERLAP=100

# Minimum chunk length (skip chunks shorter than this)
MIN_CHUNK_LENGTH=50

# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=32

# Batch size for Qdrant upserts
QDRANT_BATCH_SIZE=100

# ------------------------------------------------------------------------------
# API Server Configuration
# ------------------------------------------------------------------------------
# FastAPI host
API_HOST="0.0.0.0"

# FastAPI port
API_PORT=8000

# Environment (development, staging, production)
ENVIRONMENT="development"

# Enable debug mode (verbose logging)
DEBUG=true

# API key for authentication (optional - leave empty to disable)
API_KEY=""

# ------------------------------------------------------------------------------
# Redis Configuration (Task Queue)
# ------------------------------------------------------------------------------
# Redis connection URL
REDIS_URL="redis://localhost:6379/0"

# Redis queue name for indexing tasks
REDIS_QUEUE_NAME="indexer"

# ------------------------------------------------------------------------------
# Concurrency & Rate Limiting
# ------------------------------------------------------------------------------
# Maximum concurrent agent runs (semaphore limit)
MAX_CONCURRENT_AGENTS=8

# Rate limit: requests per minute per IP
RATE_LIMIT_PER_MINUTE=10

# Request timeout (seconds)
REQUEST_TIMEOUT=120

# ------------------------------------------------------------------------------
# Observability - Phoenix (Arize AI)
# ------------------------------------------------------------------------------
# Enable Phoenix tracing
PHOENIX_ENABLED=true

# Phoenix collector endpoint
PHOENIX_COLLECTOR_ENDPOINT="http://localhost:6006"

# Phoenix project name
PHOENIX_PROJECT_NAME="financial-10k-agentic-rag"

# ------------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------------
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL="INFO"

# Log format (json, text)
LOG_FORMAT="text"

# Log file path (optional - leave empty for stdout only)
LOG_FILE=""

# ------------------------------------------------------------------------------
# Retrieval Configuration
# ------------------------------------------------------------------------------
# Number of chunks to retrieve per query
TOP_K=5

# Minimum similarity score threshold (0.0-1.0)
MIN_SIMILARITY_SCORE=0.7

# Enable hybrid search (vector + keyword)
HYBRID_SEARCH_ENABLED=true

# Keyword search weight (0.0-1.0, only if hybrid enabled)
KEYWORD_SEARCH_WEIGHT=0.3

# ------------------------------------------------------------------------------
# Agent Configuration
# ------------------------------------------------------------------------------
# Maximum ReAct loop iterations
MAX_AGENT_ITERATIONS=10

# Enable verbose agent logging
AGENT_VERBOSE=true

# Agent system prompt customization (optional)
AGENT_SYSTEM_PROMPT=""

# ------------------------------------------------------------------------------
# Together.ai Configuration (Production LLM)
# ------------------------------------------------------------------------------
# API key for Together.ai (required for production LLM inference)
# Get your API key at: https://api.together.xyz
TOGETHER_API_KEY=""

# Main agent model for complex reasoning
# Recommended: meta-llama/Llama-3.3-70B-Instruct-Turbo, deepseek-ai/DeepSeek-V3
TOGETHER_MODEL_AGENT="meta-llama/Llama-3.3-70B-Instruct-Turbo"

# Small model for classification and quality assessment (faster, cheaper)
# Recommended: meta-llama/Llama-3.2-3B-Instruct-Turbo
TOGETHER_MODEL_SMALL="meta-llama/Llama-3.2-3B-Instruct-Turbo"

# ------------------------------------------------------------------------------
# Intent Classification
# ------------------------------------------------------------------------------
# Minimum confidence threshold for intent classification (0.0-1.0)
INTENT_CONFIDENCE_THRESHOLD=0.7

# ------------------------------------------------------------------------------
# Vector Cache Configuration
# ------------------------------------------------------------------------------
# Time-to-live for cached retrieval results (in seconds, default 5 minutes)
VECTOR_CACHE_TTL_SECONDS=300

# Maximum number of cached retrieval results
VECTOR_CACHE_MAX_SIZE=100

# ------------------------------------------------------------------------------
# Quality Assessment Gate
# ------------------------------------------------------------------------------
# Maximum retry attempts when quality assessment fails
MAX_RETRY_COUNT=3

# Enable/disable quality assessment gate
QUALITY_GATE_ENABLED=true

# ------------------------------------------------------------------------------
# Development & Testing
# ------------------------------------------------------------------------------
# Enable API docs (Swagger UI at /docs)
ENABLE_API_DOCS=true

# Enable CORS (comma-separated origins, or "*" for all)
CORS_ORIGINS="http://localhost:3000,http://localhost:8080"

# Sample companies for testing ingestion (comma-separated tickers)
SAMPLE_TICKERS="AAPL,MSFT,GOOGL"

# Sample year for testing
SAMPLE_YEAR=2023
